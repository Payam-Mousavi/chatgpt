{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Demo\n",
    "Author: Payam Mousavi\n",
    "Last updated: April 17, 2023\n",
    "Ideas were borrowed from https://github.com/gkamradt/langchain-tutorials/\n",
    "\n",
    "The tutorial focuses on loading a relatively large pdf file from the web, chunking it, creating embeddings, loading them into a vector database (i.e., pinecone) and using the OpenAI API to query the document. A simple tkinter GUI is created to interact with the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain/lib/python3.9/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import pinecone\n",
    "PINECONE_API_ENV = \"northamerica-northeast1-gcp\"\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "config = load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and chunking it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 97646 characters in your document\n",
      "Now you have 103 documents\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredPDFLoader(\"./data/Emergent_Abilities.pdf\")\n",
    "data = loader.load()\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')\n",
    "\n",
    "# Chunking:\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embeddings and storing in pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embeddings:\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(environment=PINECONE_API_ENV)\n",
    "index_name = \"langchan-demo\"\n",
    "\n",
    "# Search in Pincecone using cosine similarity:\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running queries and gettings answers from OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large language models cannot perform many tasks with above-random accuracy, such as abstract reasoning tasks like playing Chess and challenging math. They also have difficulty with multilingual emergence tasks, requiring both model scale and training data to perform well.\n"
     ]
    }
   ],
   "source": [
    "# query = \"What are examples of some emergent abilities of large languge models?\"\n",
    "# docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "query = \"What are some limitations of large language models?\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "output = chain.run(input_documents=docs, question=query)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
